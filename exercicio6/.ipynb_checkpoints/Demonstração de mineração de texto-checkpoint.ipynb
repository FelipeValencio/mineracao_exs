{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mineração de texto - recursos introdutórios"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in /home/thiago/.local/lib/python3.8/site-packages (3.7)\n",
      "Requirement already satisfied: click in /home/thiago/.local/lib/python3.8/site-packages (from nltk) (8.1.3)\n",
      "Requirement already satisfied: joblib in /home/thiago/.local/lib/python3.8/site-packages (from nltk) (1.2.0)\n",
      "Requirement already satisfied: tqdm in /home/thiago/.local/lib/python3.8/site-packages (from nltk) (4.65.0)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /home/thiago/.local/lib/python3.8/site-packages (from nltk) (2022.4.24)\n",
      "Requirement already satisfied: spacy in /home/thiago/.local/lib/python3.8/site-packages (3.5.3)\n",
      "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /home/thiago/.local/lib/python3.8/site-packages (from spacy) (1.1.1)\n",
      "Requirement already satisfied: numpy>=1.15.0 in /home/thiago/.local/lib/python3.8/site-packages (from spacy) (1.24.3)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /home/thiago/.local/lib/python3.8/site-packages (from spacy) (2.4.6)\n",
      "Requirement already satisfied: typer<0.8.0,>=0.3.0 in /home/thiago/.local/lib/python3.8/site-packages (from spacy) (0.7.0)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /home/thiago/.local/lib/python3.8/site-packages (from spacy) (2.0.7)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/thiago/.local/lib/python3.8/site-packages (from spacy) (21.3)\n",
      "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /home/thiago/.local/lib/python3.8/site-packages (from spacy) (6.3.0)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /home/thiago/.local/lib/python3.8/site-packages (from spacy) (3.0.8)\n",
      "Requirement already satisfied: setuptools in /usr/lib/python3/dist-packages (from spacy) (45.2.0)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /home/thiago/.local/lib/python3.8/site-packages (from spacy) (2.27.1)\n",
      "Requirement already satisfied: thinc<8.2.0,>=8.1.8 in /home/thiago/.local/lib/python3.8/site-packages (from spacy) (8.1.10)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /home/thiago/.local/lib/python3.8/site-packages (from spacy) (3.0.12)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /home/thiago/.local/lib/python3.8/site-packages (from spacy) (2.0.8)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4 in /home/thiago/.local/lib/python3.8/site-packages (from spacy) (1.9.2)\n",
      "Requirement already satisfied: jinja2 in /usr/lib/python3/dist-packages (from spacy) (2.10.1)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /home/thiago/.local/lib/python3.8/site-packages (from spacy) (3.3.0)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /home/thiago/.local/lib/python3.8/site-packages (from spacy) (4.65.0)\n",
      "Requirement already satisfied: pathy>=0.10.0 in /home/thiago/.local/lib/python3.8/site-packages (from spacy) (0.10.1)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /home/thiago/.local/lib/python3.8/site-packages (from spacy) (1.0.4)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /home/thiago/.local/lib/python3.8/site-packages (from spacy) (1.0.9)\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in /home/thiago/.local/lib/python3.8/site-packages (from typer<0.8.0,>=0.3.0->spacy) (8.1.3)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /home/thiago/.local/lib/python3.8/site-packages (from packaging>=20.0->spacy) (2.4.7)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/lib/python3/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (1.25.8)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/lib/python3/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2019.11.28)\n",
      "Requirement already satisfied: idna<4,>=2.5; python_version >= \"3\" in /usr/lib/python3/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2.8)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0; python_version >= \"3\" in /home/thiago/.local/lib/python3.8/site-packages (from requests<3.0.0,>=2.13.0->spacy) (2.0.12)\n",
      "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /home/thiago/.local/lib/python3.8/site-packages (from thinc<8.2.0,>=8.1.8->spacy) (0.0.4)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /home/thiago/.local/lib/python3.8/site-packages (from thinc<8.2.0,>=8.1.8->spacy) (0.7.9)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.8/dist-packages (from pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4->spacy) (4.2.0)\n",
      "Requirement already satisfied: transformers in /home/thiago/.local/lib/python3.8/site-packages (4.19.2)\n",
      "Requirement already satisfied: filelock in /home/thiago/.local/lib/python3.8/site-packages (from transformers) (3.7.0)\n",
      "Requirement already satisfied: requests in /home/thiago/.local/lib/python3.8/site-packages (from transformers) (2.27.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /home/thiago/.local/lib/python3.8/site-packages (from transformers) (2022.4.24)\n",
      "Requirement already satisfied: tokenizers!=0.11.3,<0.13,>=0.11.1 in /home/thiago/.local/lib/python3.8/site-packages (from transformers) (0.12.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.1.0 in /home/thiago/.local/lib/python3.8/site-packages (from transformers) (0.6.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/lib/python3/dist-packages (from transformers) (5.3.1)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/thiago/.local/lib/python3.8/site-packages (from transformers) (21.3)\n",
      "Requirement already satisfied: numpy>=1.17 in /home/thiago/.local/lib/python3.8/site-packages (from transformers) (1.24.3)\n",
      "Requirement already satisfied: tqdm>=4.27 in /home/thiago/.local/lib/python3.8/site-packages (from transformers) (4.65.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/lib/python3/dist-packages (from requests->transformers) (2019.11.28)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/lib/python3/dist-packages (from requests->transformers) (1.25.8)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0; python_version >= \"3\" in /home/thiago/.local/lib/python3.8/site-packages (from requests->transformers) (2.0.12)\n",
      "Requirement already satisfied: idna<4,>=2.5; python_version >= \"3\" in /usr/lib/python3/dist-packages (from requests->transformers) (2.8)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.8/dist-packages (from huggingface-hub<1.0,>=0.1.0->transformers) (4.2.0)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /home/thiago/.local/lib/python3.8/site-packages (from packaging>=20.0->transformers) (2.4.7)\n"
     ]
    }
   ],
   "source": [
    "!pip install nltk\n",
    "!pip install spacy\n",
    "!pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import nltk\n",
    "import spacy\n",
    "import string\n",
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/thiago/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /home/thiago/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package vader_lexicon to\n",
      "[nltk_data]     /home/thiago/nltk_data...\n",
      "[nltk_data]   Package vader_lexicon is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Garantir que tenha pelo menos esses dados:\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('vader_lexicon')\n",
    "\n",
    "#Você pode instalar tudo disponível executando essa linha\n",
    "#nltk.download()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessamento básico com NTLK "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Separação de sentenças"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A fazenda é muito grande.\n",
      "O Dr. da Fazenda possui vários animais.\n",
      "Aqui temos um olho d'água.\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "texto = \"A fazenda é muito grande. O Dr. da Fazenda possui vários animais. Aqui temos um olho d'água.\"\n",
    "\n",
    "sentencas = sent_tokenize(texto)\n",
    "\n",
    "for sent in sentencas:\n",
    "    print(sent)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tokenização"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A\n",
      "fazenda\n",
      "é\n",
      "muito\n",
      "grande\n",
      ".\n",
      "O\n",
      "Dr.\n",
      "da\n",
      "Fazenda\n",
      "possui\n",
      "vários\n",
      "animais\n",
      ".\n",
      "Aqui\n",
      "temos\n",
      "um\n",
      "olho\n",
      "d'água\n",
      ".\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "texto = \"A fazenda é muito grande. O Dr. da Fazenda possui vários animais. Aqui temos um olho d'água.\"\n",
    "\n",
    "palavras = word_tokenize(texto) \n",
    "\n",
    "for palavra in palavras:\n",
    "    print(palavra)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exemplificando com dados de reviews do Yelp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'I loved the customer service! Fast, but the doctor took her time to make sure I was happy in my contact lenses. Thank you!'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfCharlotte= pd.read_csv(\"reviewsTestCharlotte.csv\")\n",
    "\n",
    "reviewsCharlotte_original = dfCharlotte['text']\n",
    "reviewsCharlotte_original\n",
    "\n",
    "reviewsCharlotte_original[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(48617,)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reviewsCharlotte_original.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Convertendo para minúsculo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        i loved the customer service! fast, but the do...\n",
       "1        i've been coming here for a couple of years bu...\n",
       "2        i really liked the optometrist here. the wait ...\n",
       "3        robert and his team took great care of me when...\n",
       "4        edit: i'm lowering this review from 5-stars to...\n",
       "                               ...                        \n",
       "48612    we've now had mike out twice to fix some leaks...\n",
       "48613    love the new location lauren is always right o...\n",
       "48614    i completely ruined my hair today attempting t...\n",
       "48615    i have been going to lauren for years now and ...\n",
       "48616    always available when i need her, lauren rocks...\n",
       "Name: text, Length: 48617, dtype: object"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reviewsCharlotte_prep = reviewsCharlotte_original.str.lower()\n",
    "reviewsCharlotte_prep"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Remove pontuação"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        i loved the customer service fast but the doct...\n",
       "1        ive been coming here for a couple of years but...\n",
       "2        i really liked the optometrist here the wait w...\n",
       "3        robert and his team took great care of me when...\n",
       "4        edit im lowering this review from 5stars to 3s...\n",
       "                               ...                        \n",
       "48612    weve now had mike out twice to fix some leaks ...\n",
       "48613    love the new location lauren is always right o...\n",
       "48614    i completely ruined my hair today attempting t...\n",
       "48615    i have been going to lauren for years now and ...\n",
       "48616    always available when i need her lauren rocks ...\n",
       "Name: text, Length: 48617, dtype: object"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "PONTUACAO = string.punctuation\n",
    "\n",
    "def remove_pontuacao(text):\n",
    "    return text.translate(str.maketrans('', '', PONTUACAO))\n",
    "\n",
    "reviewsCharlotte_prep = reviewsCharlotte_prep.apply(lambda text: remove_pontuacao(text))\n",
    "reviewsCharlotte_prep"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Remove stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0        loved customer service fast doctor took time m...\n",
      "1        ive coming couple years optometrist business b...\n",
      "2        really liked optometrist wait little longer wo...\n",
      "3        robert team took great care repairing car back...\n",
      "4        edit im lowering review 5stars 3stars wrote or...\n",
      "                               ...                        \n",
      "48612    weve mike twice fix leaks pex pipes first time...\n",
      "48613    love new location lauren always right point lo...\n",
      "48614    completely ruined hair today attempting highli...\n",
      "48615    going lauren years would never see another hai...\n",
      "48616    always available need lauren rocks particular ...\n",
      "Name: text, Length: 48617, dtype: object\n",
      "{'having', \"hadn't\", \"you'll\", 't', 'under', \"weren't\", 'during', 'both', 'further', 'm', 'been', 'y', 'itself', 'same', 'wouldn', 'from', 'such', 'an', 'doesn', 'was', 'to', 'at', 'some', 'in', 'who', 'be', 'what', 'or', 'as', 'll', 'myself', 's', 'how', \"don't\", 'won', 'few', 'of', 'is', 'hadn', 'me', 'here', 'own', 'do', \"haven't\", 'any', 'being', 'didn', 'mustn', \"didn't\", 'out', 'his', 'yourselves', 'up', 'on', \"she's\", 'no', 'most', 'o', 'will', 'wasn', 'him', 'mightn', \"isn't\", 'between', 'you', 'after', 'can', 'our', \"mustn't\", 'ourselves', 'against', \"aren't\", \"doesn't\", \"won't\", 'about', 'himself', 'their', 'into', 'then', 'have', 'themselves', 'were', 've', \"wouldn't\", \"needn't\", 'these', 'this', 'why', \"you've\", 'hasn', 'because', 'ain', 'ma', 'couldn', 'too', 're', 'until', 'for', 'once', 'other', 'hers', \"hasn't\", 'had', 'the', 'by', 'shouldn', 'them', 'we', 'just', 'while', 'through', 'it', 'doing', 'more', 'its', 'but', 'nor', \"wasn't\", 'has', 'all', 'and', 'each', 'should', 'when', \"shouldn't\", 'yourself', 'yours', 'that', 'before', 'if', 'whom', 'above', \"couldn't\", 'your', \"mightn't\", 'i', \"you'd\", 'herself', 'she', 'down', 'haven', 'weren', 'a', 'does', 'aren', \"you're\", \"shan't\", 'not', 'ours', 'which', 'am', \"should've\", 'than', 'isn', 'shan', \"that'll\", 'now', 'don', 'they', 'are', 'again', 'so', 'did', 'with', 'he', 'those', 'only', \"it's\", 'below', 'where', 'd', 'off', 'there', 'needn', 'very', 'my', 'her', 'over', 'theirs'}\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "\n",
    "STOPWORDS = set(stopwords.words('english'))\n",
    "\n",
    "def remove_stopwords(text):\n",
    "    return \" \".join([word for word in str(text).split() if word not in STOPWORDS])\n",
    "\n",
    "reviewsCharlotte_prep = reviewsCharlotte_prep.apply(lambda text: remove_stopwords(text))\n",
    "print(reviewsCharlotte_prep)\n",
    "\n",
    "print(STOPWORDS)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        love custom servic fast doctor took time make ...\n",
       "1        ive come coupl year optometrist busi busi yelp...\n",
       "2        realli like optometrist wait littl longer woul...\n",
       "3        robert team took great care repair car back ca...\n",
       "4        edit im lower review 5star 3star wrote origin ...\n",
       "                               ...                        \n",
       "48612    weve mike twice fix leak pex pipe first time l...\n",
       "48613    love new locat lauren alway right point love h...\n",
       "48614    complet ruin hair today attempt highlight than...\n",
       "48615    go lauren year would never see anoth hairdress...\n",
       "48616    alway avail need lauren rock particular red ha...\n",
       "Name: text, Length: 48617, dtype: object"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.stem.porter import PorterStemmer\n",
    "\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "def stem_words(text):\n",
    "    return \" \".join([stemmer.stem(word) for word in text.split()])\n",
    "\n",
    "reviewsCharlotte_prep= reviewsCharlotte_prep.apply(lambda text: stem_words(text))\n",
    "reviewsCharlotte_prep"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Topic Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models.ldamodel import LdaModel\n",
    "from gensim.corpora.dictionary import Dictionary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sem pré-processamento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_orig = [d.split() for d in reviewsCharlotte_original]\n",
    "dictionary_orig = Dictionary(dataset_orig)\n",
    "corpus_orig = [dictionary_orig.doc2bow(doc) for doc in dataset_orig]\n",
    "\n",
    "model_original =LdaModel(corpus=corpus_orig, id2word=dictionary_orig, num_topics=10, iterations=100, passes=5,random_state=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0, '0.023*\"games\" + 0.010*\"cookies\" + 0.009*\"Asked\" + 0.005*\"Rooms\" + 0.005*\"delivered.\" + 0.004*\"purpose\"')\n",
      "---\n",
      "(1, '0.040*\"I\" + 0.036*\"to\" + 0.033*\"the\" + 0.020*\"a\" + 0.017*\"you\" + 0.016*\"that\"')\n",
      "---\n",
      "(2, '0.021*\"massage\" + 0.008*\"Taste\" + 0.006*\"sake\" + 0.006*\"Still,\" + 0.005*\"Suites\" + 0.005*\"Avoid\"')\n",
      "---\n",
      "(3, '0.005*\"Ice\" + 0.004*\"mold\" + 0.004*\"Awful\" + 0.003*\"pleasure.\" + 0.003*\"this?\" + 0.003*\"Cesar\"')\n",
      "---\n",
      "(4, '0.050*\"the\" + 0.040*\"and\" + 0.039*\"was\" + 0.030*\"I\" + 0.026*\"a\" + 0.016*\"The\"')\n",
      "---\n",
      "(5, '0.015*\"Thai\" + 0.015*\"ribs\" + 0.008*\"tire\" + 0.007*\"pork,\" + 0.007*\"Taco\" + 0.006*\"Fantastic\"')\n",
      "---\n",
      "(6, '0.012*\"Hall\" + 0.010*\"Western\" + 0.004*\"tipped\" + 0.004*\"Yeah,\" + 0.003*\"watch.\" + 0.002*\":).\"')\n",
      "---\n",
      "(7, '0.045*\"hair\" + 0.020*\"salon\" + 0.011*\"cut\" + 0.010*\"color\" + 0.010*\"polish\" + 0.008*\"pedicure\"')\n",
      "---\n",
      "(8, '0.046*\"the\" + 0.043*\"and\" + 0.039*\"a\" + 0.029*\"is\" + 0.023*\"to\" + 0.021*\"of\"')\n",
      "---\n",
      "(9, '0.046*\"the\" + 0.044*\"and\" + 0.041*\"to\" + 0.040*\"I\" + 0.038*\"was\" + 0.022*\"a\"')\n",
      "---\n"
     ]
    }
   ],
   "source": [
    "for i in  model_original.show_topics(num_topics=10, num_words=6, log=False):\n",
    "    print(i)\n",
    "    print('---')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Com pré-processamento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_prep = [d.split() for d in reviewsCharlotte_prep]\n",
    "dictionary_prep = Dictionary(dataset_prep)\n",
    "corpus_prep = [dictionary_prep.doc2bow(doc) for doc in dataset_prep]\n",
    "\n",
    "model_prep =LdaModel(corpus=corpus_prep, id2word=dictionary_prep, num_topics=10, iterations=100, passes=5,random_state=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0, '0.024*\"beer\" + 0.014*\"bar\" + 0.013*\"place\" + 0.013*\"great\" + 0.012*\"good\" + 0.010*\"nice\"')\n",
      "---\n",
      "(1, '0.023*\"order\" + 0.023*\"food\" + 0.015*\"time\" + 0.015*\"wait\" + 0.014*\"us\" + 0.013*\"minut\"')\n",
      "---\n",
      "(2, '0.021*\"breakfast\" + 0.018*\"coffe\" + 0.017*\"egg\" + 0.017*\"brunch\" + 0.014*\"ice\" + 0.013*\"cream\"')\n",
      "---\n",
      "(3, '0.044*\"great\" + 0.035*\"food\" + 0.031*\"place\" + 0.025*\"servic\" + 0.019*\"good\" + 0.018*\"love\"')\n",
      "---\n",
      "(4, '0.022*\"call\" + 0.016*\"would\" + 0.012*\"told\" + 0.011*\"custom\" + 0.011*\"said\" + 0.010*\"servic\"')\n",
      "---\n",
      "(5, '0.043*\"sub\" + 0.027*\"gift\" + 0.023*\"japanes\" + 0.022*\"groceri\" + 0.021*\"jerk\" + 0.018*\"fuel\"')\n",
      "---\n",
      "(6, '0.020*\"good\" + 0.019*\"burger\" + 0.015*\"chicken\" + 0.014*\"fri\" + 0.014*\"order\" + 0.011*\"like\"')\n",
      "---\n",
      "(7, '0.016*\"like\" + 0.014*\"get\" + 0.012*\"go\" + 0.010*\"im\" + 0.010*\"look\" + 0.010*\"place\"')\n",
      "---\n",
      "(8, '0.123*\"airport\" + 0.083*\"room\" + 0.072*\"hotel\" + 0.053*\"stay\" + 0.050*\"taco\" + 0.030*\"flight\"')\n",
      "---\n",
      "(9, '0.015*\"car\" + 0.014*\"help\" + 0.011*\"work\" + 0.011*\"store\" + 0.010*\"need\" + 0.009*\"staff\"')\n",
      "---\n"
     ]
    }
   ],
   "source": [
    "for i in  model_prep.show_topics(num_topics=10, num_words=6, log=False):\n",
    "    print(i)\n",
    "    print('---')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Probabilidades dos tópicos em cada documento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<gensim.interfaces.TransformedCorpus at 0x7fe0b9785cd0>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docsProbabilities = model_prep.get_document_topics(corpus_prep, minimum_probability=0)\n",
    "\n",
    "docsProbabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 0.0071519343), (1, 0.15872245), (2, 0.007151945), (3, 0.25934404), (4, 0.00715396), (5, 0.0071514593), (6, 0.007151999), (7, 0.007152517), (8, 0.0071514593), (9, 0.5318682)]\n",
      "[(0, 0.0016159642), (1, 0.0016160049), (2, 0.0016156161), (3, 0.001615824), (4, 0.20321089), (5, 0.0016154525), (6, 0.1409837), (7, 0.001615989), (8, 0.0016155083), (9, 0.64449507)]\n",
      "[(0, 0.001852898), (1, 0.00185335), (2, 0.0018526925), (3, 0.0018531331), (4, 0.0018529615), (5, 0.0018524572), (6, 0.0018528634), (7, 0.5231479), (8, 0.0018524554), (9, 0.46202928)]\n",
      "[(0, 0.00294224), (1, 0.0029428527), (2, 0.0029422138), (3, 0.0029421574), (4, 0.11584241), (5, 0.0029418338), (6, 0.0029423486), (7, 0.0029422585), (8, 0.0029418422), (9, 0.86061984)]\n",
      "[(0, 0.08094109), (1, 0.00048701806), (2, 0.00048694856), (3, 0.0004870454), (4, 0.5226361), (5, 0.005358452), (6, 0.029970376), (7, 0.00048702172), (8, 0.00048686858), (9, 0.35865906)]\n",
      "[(0, 0.33742812), (1, 0.007703148), (2, 0.0077025583), (3, 0.31229982), (4, 0.007704205), (5, 0.007702004), (6, 0.007703077), (7, 0.0077030244), (8, 0.084665366), (9, 0.21938865)]\n",
      "[(0, 0.00156635), (1, 0.0015663021), (2, 0.0015661189), (3, 0.0015664153), (4, 0.31293058), (5, 0.0015660233), (6, 0.0015662356), (7, 0.22381365), (8, 0.0015660231), (9, 0.45229232)]\n",
      "[(0, 0.0023813616), (1, 0.0023816754), (2, 0.002381419), (3, 0.002381788), (4, 0.16609895), (5, 0.00238124), (6, 0.0023815678), (7, 0.002381521), (8, 0.0023812435), (9, 0.81484926)]\n",
      "[(0, 0.282846), (1, 0.0012070352), (2, 0.0012067542), (3, 0.0012069045), (4, 0.36821643), (5, 0.013243909), (6, 0.0012067981), (7, 0.0012069877), (8, 0.0012066998), (9, 0.3284525)]\n",
      "[(0, 0.0029450746), (1, 0.0029449915), (2, 0.00294481), (3, 0.0029451917), (4, 0.38797656), (5, 0.002944639), (6, 0.002945068), (7, 0.0029451821), (8, 0.0029446434), (9, 0.58846384)]\n"
     ]
    }
   ],
   "source": [
    "#Probabilidade de cada tópico por documento. Tupla (topico, probabilidade)\n",
    "for docDistribution in docsProbabilities[0:10]:\n",
    "    print(docDistribution)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Análise de Sentimentos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Utilizando o VADER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'neg': 0.0, 'neu': 0.385, 'pos': 0.615, 'compound': 0.7489}\n"
     ]
    }
   ],
   "source": [
    "from nltk.sentiment import SentimentIntensityAnalyzer\n",
    "\n",
    "sent = SentimentIntensityAnalyzer()\n",
    "\n",
    "print(sent.polarity_scores(\"Yes! Data mining is very powerful!\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A pontuação composta (compound) é calculada de tal maneira que representa a polaridade de -1 a 1, onde +1 é o mais positivo e -1 é o mais negativo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Utilizando o Textblob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting textblob\n",
      "  Using cached textblob-0.17.1-py2.py3-none-any.whl (636 kB)\n",
      "Requirement already satisfied, skipping upgrade: nltk>=3.1; python_version >= \"3\" in /home/thiago/.local/lib/python3.8/site-packages (from textblob) (3.7)\n",
      "Requirement already satisfied, skipping upgrade: regex>=2021.8.3 in /home/thiago/.local/lib/python3.8/site-packages (from nltk>=3.1; python_version >= \"3\"->textblob) (2022.4.24)\n",
      "Requirement already satisfied, skipping upgrade: tqdm in /home/thiago/.local/lib/python3.8/site-packages (from nltk>=3.1; python_version >= \"3\"->textblob) (4.61.1)\n",
      "Requirement already satisfied, skipping upgrade: click in /usr/lib/python3/dist-packages (from nltk>=3.1; python_version >= \"3\"->textblob) (7.0)\n",
      "Requirement already satisfied, skipping upgrade: joblib in /home/thiago/.local/lib/python3.8/site-packages (from nltk>=3.1; python_version >= \"3\"->textblob) (1.1.0)\n",
      "Installing collected packages: textblob\n",
      "Successfully installed textblob-0.17.1\n"
     ]
    }
   ],
   "source": [
    "!pip install -U textblob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sentiment(polarity=0.48750000000000004, subjectivity=1.0)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from textblob import TextBlob\n",
    "\n",
    "frase = TextBlob(\"Yes! Data mining is very powerful!\")\n",
    "frase.sentiment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exemplo com os reviews do Yelp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I loved the customer service! Fast, but the doctor took her time to make sure I was happy in my contact lenses. Thank you!\n",
      "----\n",
      "Robert and his team took great care of me when repairing my car back up camera. The team was able to identify the issue and resolve it for a low cost option. The team could have taken advantage of me and oversold equipment that wasn't needed, however they were very upfront about the root cause of the issue and I was very pleased with the repair and the repair cost !\n",
      "----\n",
      "Edit: I'm lowering this review from 5-stars to 3-stars.  I wrote the original review prematurely. After having been around inside my car a bit I have noticed broken rivets & clips and pieces of carpet were never put back where they should have been. Also, many leftover pieces of wire and debris were left in the crevices and sides of the seat. My biggest issue is every time I turn my car on I hear a pop come from the JL subwoofer that was bought and installed at Freeman's. I brought this up to Joel (manager) and he told me to turn my stereo off before I turn the car off but the pop just returned when I turn the stereo on. I'm having to get these issues fixed elsewhere. Other than what I just mentioned everything else was done well and done just as I had requested. \n",
      "\n",
      "\n",
      "______________________________________________\n",
      "I would recommend Freeman's to anyone. I worked with Joel during my experience there and he was very helpful and a straight shooter with me.\n",
      "\n",
      "I called in one day and spoke to him about my issue and he briefly discussed over the phone then told me to come into to discuss further. I drove up there and he explained and demonstrated the difference of sound with various equipment. This was very helpful and informative in my decision of what equipment I was going to need. \n",
      "\n",
      "I had him make me up a quote, everything was priced great. I compared his pricing with online retailers. His original quote contained the same price for the sub on Crutchfield. Then I spoke to him again and I asked him to match Crutchfield price for the amplifier and he was able to go lower than Crutchfield.  Install price was very very reasonable compared to a few other places I looked up. My total invoice when I paid came in $5 less than my quote which I was very pleased about, no extra nonsense added later.\n",
      "\n",
      "Install was great. I had a few minor requests which Joel didn't hesitate to tell me they could accomplish. I didn't notice any broken parts of anything from installation. Everything looks like it did when I dropped my car off, except when you turn up the volume! Sounded exactly the way I had described to Joel what I was looking for.\n",
      "\n",
      "One of my favorite parts was the bass control knob placement. I had only expected it to be up under the dash, but what they did was awesome. They were able to place it on the center console of my Jeep Wrangler in a vacant spot under the arm rest that makes it look as if the knob was OEM. Such a cool added feature and I've realized how much more convenient it is.\n",
      "\n",
      "Again, I would recommend Freeman's.\n",
      "----\n",
      "Jason and Roger provided excellent service...and they're very military friendly. I would definitely return. Thank you gentlemen!\n",
      "----\n",
      "I had a back up camera installed and it was the smoothest, cleanest deal I've ever made!  Robert was quick to return my initial call with very concise information regarding the equipment needed and the exact costs to perform the installation.  I took the car in at the time arranged and several hours later I was called to pick it up.  It works perfectly!\n",
      "\n",
      "I am happy with the equipment, the service and the professional manner in which the entire deal was made and executed!\n",
      "----\n"
     ]
    }
   ],
   "source": [
    "def verifica_MuitoPositivo(review):\n",
    "    \n",
    "    return sent.polarity_scores(review)[\"compound\"] > 0.8\n",
    "\n",
    "\n",
    "for review in reviewsCharlotte_original[0:10]:\n",
    "    \n",
    "    if verifica_MuitoPositivo(review):\n",
    "        print(review)\n",
    "        print('----')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transformers - Frases similares com sBert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "02f67a5fc38248129f5fb8e76766df14",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/1.18k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8747bad2038b462da836b0b72e241e15",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/190 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b0e3b62137ef449e86922807722975af",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/10.2k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "71188c32da5e4726b656c434072fcb60",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/612 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "078eb747356143b181428dfa645337c4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/116 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "77b54444bede421680e816f580f96c4e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/39.3k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5745e4d4be7c485e968d5d3ebda65ae9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/349 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "22678be045d048a3a8eb2d13735cc384",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/90.9M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f3ef181c778d44458daaebec58502f0b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/53.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "234ce66974694d6fa5edd26779d3d088",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/112 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c66b79f6a4304047a5b4f6fecd409c05",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/466k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f839a9e9435a401295da4c91396238d8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/350 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ed60da7464d843009a44313638a467e8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/13.2k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "57b93703b0c24ea08abadfcc8a6f1c90",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#!pip install -U sentence-transformers\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "\n",
    "model_RobLarge = SentenceTransformer('all-MiniLM-L6-v2', device='cuda')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfTweetsTrump = pd.read_csv(\"tweets_trump.csv\")\n",
    "\n",
    "#Usando somente com os 10000 primeiros -\n",
    "embeddingsTweets_ROBbase = model_RobLarge.encode(dfTweetsTrump.loc[0:10000,'text'], convert_to_tensor=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddingsTweets_ROBbase\n",
    "matSimi = cosine_similarity(embeddingsTweets_ROBbase.cpu().detach().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "maior = 0\n",
    "count = 0\n",
    "docID = -1\n",
    "docTarget = 120\n",
    "\n",
    "for i in matSimi[docTarget,:]:\n",
    "    if count == docTarget:\n",
    "        count +=1\n",
    "        continue\n",
    "        \n",
    "    if i> maior:\n",
    "        maior = i\n",
    "        docID = count\n",
    "    \n",
    "    count+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "****target***\n",
      "RT @Claire_FOX5: #BREAKING:  @GaSecofState's office confirms Floyd County has found 2,600 ballots during audit.  Says Sec. Raffensperger wa…\n",
      "\n",
      "*** mais similar ***\n",
      "Investigators Dispatched After Fulton County Discovers ‘Issue‘ with Ballot Reporting https://t.co/lShmKksQ0O via @BreitbartNews\n",
      "0.61102223\n",
      "2582\n"
     ]
    }
   ],
   "source": [
    "print('****target***')\n",
    "print(dfTweetsTrump.loc[docTarget,'text'])\n",
    "print()\n",
    "print(\"*** mais similar ***\")\n",
    "print(dfTweetsTrump.loc[docID,'text'])\n",
    "\n",
    "print(maior)\n",
    "print(docID)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Outros exemplos com Transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classificação - zero-shot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "classifier = pipeline(\"zero-shot-classification\", model='valhalla/distilbart-mnli-12-1')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'sequence': 'This is the Data Mining course from CPGEI',\n",
       "  'labels': ['computer', 'education', 'finance', 'politics'],\n",
       "  'scores': [0.5431435108184814,\n",
       "   0.35852858424186707,\n",
       "   0.058774080127477646,\n",
       "   0.039553768932819366]},\n",
       " {'sequence': 'I have a very good stock option, gonna be rich!',\n",
       "  'labels': ['finance', 'computer', 'education', 'politics'],\n",
       "  'scores': [0.882773756980896,\n",
       "   0.04628705978393555,\n",
       "   0.046192556619644165,\n",
       "   0.02474653534591198]}]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier(\n",
    "    [\"This is the Data Mining course from CPGEI\",\n",
    "     \"I have a very good stock option, gonna be rich!\"],\n",
    "    \n",
    "    candidate_labels=[\"education\", \"politics\", \"finance\", \"computer\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NER - Named Entity Recognition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to dbmdz/bert-large-cased-finetuned-conll03-english (https://huggingface.co/dbmdz/bert-large-cased-finetuned-conll03-english)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2f2665657d87481098d68255b20cb910",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/998 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b889bf9b891a435b88095ae16ae90f4a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/1.24G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "68e370f062fa4026a953b5c6d7dd55f3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/60.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fc7664d1299a4c4eb755001077dbcaf3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/208k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/thiago/.local/lib/python3.8/site-packages/transformers/pipelines/token_classification.py:135: UserWarning: `grouped_entities` is deprecated and will be removed in version v5.0.0, defaulted to `aggregation_strategy=\"AggregationStrategy.SIMPLE\"` instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "ner = pipeline(\"ner\", grouped_entities=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'entity_group': 'PER',\n",
       "  'score': 0.9912441,\n",
       "  'word': 'Thiago',\n",
       "  'start': 5,\n",
       "  'end': 11},\n",
       " {'entity_group': 'ORG',\n",
       "  'score': 0.9959862,\n",
       "  'word': 'CPGEI',\n",
       "  'start': 39,\n",
       "  'end': 44}]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ner(\"I am Thiago and a teach Data Mining at CPGEI.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## exemplos de transformers: https://huggingface.co/models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## spacy transformers: https://spacy.io/usage/embeddings-transformers#transformers"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dmclass",
   "language": "python",
   "name": "dmclass"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
